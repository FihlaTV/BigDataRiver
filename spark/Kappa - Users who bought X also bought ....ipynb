{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.cassandra.connection.host=cassandra --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,com.datastax.spark:spark-cassandra-connector_2.11:2.0.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"BigDataRiver\")\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, 20)\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kafkaStream = KafkaUtils.createDirectStream(ssc, ['bdr'], {\"metadata.broker.list\": 'kafka:9092'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = kafkaStream.map(lambda v: v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product\", LongType(), False),\n",
    "    StructField(\"other_product\", LongType(), False),\n",
    "    StructField(\"count\", LongType(), False)\n",
    "])\n",
    "\n",
    "def add(p1,o1,c1,p2,o2,c2):\n",
    "  if p2 is None:\n",
    "    return (p1,o1,c1)\n",
    "  elif p1 is None:\n",
    "    return (p2,o2,c2)\n",
    "  else: \n",
    "    return (p1,o1,c1+c2)\n",
    "\n",
    "add_udf = F.udf(add, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bdr(rdd):\n",
    "    df0 = sql.read.json(rdd)\n",
    "    if(len(df0.columns)):\n",
    "        df = df0.select('user_id', 'product').cache()\n",
    "        users = df.toDF('user_id', 'other_product')\n",
    "        #stream products\n",
    "        s_products = df.join(users, users['user_id'] == df['user_id'], 'inner').filter(\"`product` != `other_product`\").select('product','other_product').groupby('product','other_product').count().toDF(\"p1\",\"o1\",\"c1\")\n",
    "        #products stored in cassandra\n",
    "        c_products = sql.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"all_products\", keyspace=\"bdr\").load().toDF(\"p2\",\"o2\",\"c2\")\n",
    "        #join both\n",
    "        products = s_products.join(c_products, (s_products['p1'] == c_products['p2']) & (s_products['o1'] == c_products['o2']), 'outer')\n",
    "        new_products = products.withColumn('add_column', add_udf(products['p1'],products['o1'],products['c1'],products['p2'],products['o2'],products['c2'])).select(\"add_column.product\",\"add_column.other_product\",\"add_column.count\").cache()\n",
    "        #store back the latest counts\n",
    "        new_products.write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"all_products\", keyspace=\"bdr\").save()\n",
    "        \n",
    "        #now calculate the top products\n",
    "        p_o_rdd = new_products.rdd.map(lambda r: (r[0], (r[1], r[2]))).groupByKey().mapValues(lambda x: map(lambda z: z[0], sorted(x, reverse=True, key=lambda y: y[1])[:5]))\n",
    "        schema = StructType([\n",
    "            StructField(\"product\", LongType(), False),\n",
    "            StructField(\"other_products\", ArrayType(LongType()), False)\n",
    "        ])\n",
    "        sql.createDataFrame(p_o_rdd, schema).write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"top_other_products\", keyspace=\"bdr\").save()\n",
    "        \n",
    "    else:\n",
    "        print \"Empty\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.foreachRDD(lambda rdd: bdr(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
